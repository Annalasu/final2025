<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>简答题</title>
  <style>
    body {
      font-family: "微软雅黑", sans-serif;
      line-height: 1.6;
      margin: 20px;
      color: #333;
    }

    h1,
    h2,
    h3 {
      color: #007acc;
    }

    .card {
      background-color: #fdfdfd;
      border: 1px solid #eee;
      border-radius: 8px;
      padding: 15px;
      margin-bottom: 20px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }

    .card h3 {
      margin-top: 0;
    }

    .card p,
    .card ul {
      margin: 8px 0;
    }

    .card ul {
      list-style: disc;
      margin-left: 20px;
    }

    .code {
      background-color: #f5f5f5;
      padding: 5px 8px;
      border-radius: 4px;
      color: #e83e8c;
      font-family: "Consolas", monospace;
    }
  </style>
</head>

<body>

  <h1>简答题</h1>

  <!-- 一、机器学习概述 -->
  <div class="card">
    <h2>一、机器学习概述</h2>
    <p>机器学习是利用统计学方法确定变量间相互依赖关系的领域。它通过各种算法使计算机系统能够从数据中学习，而无需进行明确的编程。构建一个完整的机器学习算法通常需要数据、模型以及评估和训练/验证或性能度量准则等要素。</p>
    <ul>
      <li><strong>分类（Classification）</strong>: 分类是典型的监督学习任务。它的目标是通过训练样本构建合适的分类器，将数据集中的样本划分到不同的类别中。如果只有两个类别，则称为二值分类或二分类（通常用+1和-1指代），多于两个类别则称为多值分类。分类与聚类的主要区别在于，分类算法是有训练样本的，属于监督学习，而聚类所要求划分的类是未知的，通常理解为无监督学习.</li>
      <li><strong>回归（Regression）</strong>: 回归是利用最小平方函数对一个或多个自变量和因变量之间关系进行建模。它旨在用一条曲线对数据点进行拟合，这条曲线被称为最佳拟合曲线，这个拟合过程称为回归。线性回归解决的是回归问题.</li>
      <li><strong>聚类（Clustering）</strong>: 聚类是无监督学习的一种类型。它的目的是识别数据中未知的结构，将集合划分成由相似对象组成的多个类的过程。聚类的宗旨是实现“类内距离最小化”和“类间距离最大化”。聚类所要求的类是未知的，没有预定义的类和样本，因此是一种无监督的数据挖掘任务。</li>
      <li><strong>过拟合（Overfitting）和欠拟合（Underfitting）</strong>:
        <ul>
          <li><strong>过拟合</strong>是指模型在训练集上表现非常好，但在测试集上表现很差。这意味着模型的训练误差低但泛化误差高。过拟合发生时，模型可能将训练样本自身的特点当作了所有潜在样本都具有的一般性质，导致泛化性能下降。解决过拟合的方法包括：增加训练数据量、对模型进行裁剪、正则化、预剪枝等。</li>
          <li><strong>欠拟合</strong>是指模型在训练集上表现非常差，但在测试集上表现也差。通常是由于模型不能对训练集进行拟合或训练迭代次数太少。解决欠拟合的方法包括：增加分类属性的数量、选取合适的分类属性等。</li>
        </ul>
      </li>
      <li><strong>监督学习（Supervised Learning）</strong>: 监督学习的数据点有已知的结果（即有标签数据）。典型的监督学习算法包括分类和逻辑回归.</li>
      <li><strong>无监督学习（Unsupervised Learning）</strong>: 无监督学习的数据点没有已知结果（即无标记数据）。其目标是识别数据中未知的结构。降维和聚类是典型的无监督学习算法.</li>
      <li><strong>半监督学习（Semi-supervised Learning）</strong>: 机器学习根据样本集合中是否包含标签以及半包含标签的多少，可以分为监督学习、无监督学习和半监督学习.</li>
      <li><strong>机器学习的一般流程</strong>: 机器学习算法的构建需要数据、模型以及训练和验证三个方面的要素. 训练和泛化误差之间的差异越小，说明模型的泛化性能越好.</li>
      <li><strong>正则化（Regularization）</strong>: 正则化是一种解决模型过拟合的方法。L1正则化倾向于产生稀疏的权重矩阵，可以将不重要的特征参数设置为0，因此可以作为特征选择的一种方式。L2正则化在所有特征都对预测结果有贡献但需要控制模型复杂度以防止过拟合时是一个好的选择。</li>
      <li><strong>模型评价（Model Evaluation）</strong>: 混淆矩阵是大多数评价指标的基础，它主要包括真阴率（TN）、假阳率（FP）、假阴率（FN）和真阳率（TP）四部分信息。
        <ul>
          <li><strong>准确率（Accuracy）</strong>: 预测正确的结果占总样本的百分比。在样本不平衡的情况下，准确率不能作为很好的指标来衡量结果.</li>
          <li><strong>精确率（Precision）</strong>: 又叫查准率，表示在所有被预测为正的样本中实际为正的样本的概率.</li>
          <li><strong>召回率（Recall）</strong>: 又叫查全率，表示在实际为正的样本中被预测为正样本的概率. 召回率一般应用于宁可错杀一千，绝不放过一个的场景.</li>
          <li><strong>F1-score</strong>: 是精确率和召回率的调和平均值，综合衡量模型的性能.</li>
          <li><strong>PR曲线</strong>: 以精确率为纵坐标，召回率为横坐标做出的曲线.</li>
          <li><strong>ROC曲线与AUC</strong>: ROC曲线以假阳率（FPR）为横轴，真阳率（TPR）为纵轴. AUC（Area Under Curve）是ROC曲线下方的面积，用于衡量分类器的整体性能.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 二、线性回归、最小二乘法、逻辑回归的特点 -->
  <div class="card">
    <h2>二、线性回归、最小二乘法、逻辑回归的特点</h2>
    <ul>
      <li><strong>线性回归（Linear Regression）</strong>:
        <ul>
          <li>它研究的是自变量与因变量之间的线性关系。</li>
          <li>目标是求解权重（<span class="code">𝒘</span>）和偏置（b），使得预测值与真实值尽可能接近。</li>
          <li>适用于对连续型目标变量进行建模与预测.</li>
        </ul>
      </li>
      <li><strong>最小二乘法（Least Squares Method）</strong>:
        <ul>
          <li>它是求解线性回归模型的基本方法，是一个不带条件的最优化问题。</li>
          <li>优化目标是让整个样本集合上的预测值与真实值之间的欧式距离之和最小.</li>
          <li><strong>局限性</strong>: 需要计算逆矩阵，可能不存在或计算耗时；如果拟合函数不是线性的，无法直接使用。它对异常值很敏感，异常值可能导致拟合模型出现很大变化，预测结果不准确，从而导致分类错误.</li>
        </ul>
      </li>
      <li><strong>逻辑回归（Logistic Regression）</strong>:
        <ul>
          <li><strong>特点</strong>:
            <ul>
              <li>解决的是分类问题.</li>
              <li>它是对线性回归对异常数据敏感的不足进行了优化改进。通过使用Sigmoid函数将线性回归模型输出的连续值进行“掰弯”离散化，使其对异常值不那么敏感.</li>
              <li>是一种广义线性回归，通过回归对数几率的方式将线性回归应用于分类任务.</li>
              <li>对数几率（logistic）函数是任意阶可导的凸函数，具有很好的数学性质.</li>
              <li>简单易用，原理和计算过程相对容易理解，方便快速上手.</li>
              <li>可解释性强，模型参数可以很容易地解释为对数几率，方便理解和解释预测结果.</li>
              <li>不能用于解决非线性回归问题.</li>
            </ul>
          </li>
          <li><strong>求解过程</strong>: 找一个合适的预测分类函数（h函数），构造一个损失函数（<span class="code">𝐽(𝜃)</span>）表示预测输出与真实类别之间的偏差，然后使用梯度下降法找到损失函数的最小值.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 三、k - 近邻算法（K-Nearest Neighbors, KNN） -->
  <div class="card">
    <h2>三、k - 近邻算法（K-Nearest Neighbors, KNN）</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>是机器学习算法，也是监督学习算法.</li>
          <li>核心思想是：给定测试样本，基于特定的距离度量方式找到与训练集中最接近的K个样本，然后基于这K个样本的类别进行预测.</li>
          <li>基本要素包括：距离度量（如欧氏距离、曼哈顿距离、余弦距离等）、K值选择和分类决策规则.</li>
          <li>K的选择对分类结果有影响. 随着K值的增大，决策边界会越来越简单.</li>
          <li>距离计算方法不同，效果也可能有显著差别.</li>
          <li>可以用来解决回归问题.</li>
          <li>一般使用投票法进行分类任务.</li>
          <li>当训练样本数量很大时，计算开销高。可以通过引入邻域权值、特征降维与模式融合、以及快速检索（如kd树）等方法进行优化.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 四、决策树（Decision Tree） -->
  <div class="card">
    <h2>四、决策树（Decision Tree）</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>是一种用于分类和回归的机器学习算法.</li>
          <li>通过一系列的决策规则将数据逐步划分，最终形成类似于树状结构的模型.</li>
          <li>由决策结点、分支和叶子结点组成.</li>
          <li>具有很强的数据拟合能力，容易产生过拟合.</li>
          <li>通过贪心策略挑选最优属性.</li>
          <li>不能保证找到全局最优解.</li>
          <li><strong>主要构建方法</strong>:
            <ul>
              <li><strong>ID3算法</strong>: 最早用于决策树模型的特征选择指标，也是ID3算法的核心。根据信息论的信息增益评估和选择特征，每次选择信息增益最大的候选特征作为判断模块。ID3不能处理连续分布的数据特征，且对可取数值多的属性有偏好.</li>
              <li><strong>C4.5算法</strong>: 引入信息增益率作为度量，克服了信息增益偏向于选择特征值个数较多分支的不足。能够完成对连续属性的离散化处理和对不完整数据的处理.</li>
              <li><strong>CART算法</strong>: 采用二分循环分割的方法，每次将样本集划分为两个子样本集，生成二叉树。在分支处理中使用的度量指标是Gini指数. 既可用于分类，也可用于预测（回归）.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 五、朴素贝叶斯（Naive Bayes） -->
  <div class="card">
    <h2>五、朴素贝叶斯（Naive Bayes）</h2>
    <ul>
      <li><strong>朴素贝叶斯分类器的假设条件</strong>:
        <ul>
          <li>朴素贝叶斯算法是基于贝叶斯定理与<strong>特征属性独立假设</strong>的分类方法。</li>
          <li>该算法采用了“特征属性独立性假设”的方法，即对已知类别，假设所有特征属性相互独立。换言之，假设每个特征属性独立地对分类结果发生影响。</li>
        </ul>
      </li>
      <li><strong>训练过程</strong>:
        <ul>
          <li><strong>基本原理</strong>: 通过考虑特征概率来预测分类。</li>
          <li><strong>步骤</strong>:
            <ol>
              <li>确定特征属性，获取训练样本集合.</li>
              <li>计算各类别的先验概率P(Y=Ck).</li>
              <li>计算各类别下各特征属性Xj的条件概率.</li>
              <li>计算各类别的后验概率P(Y=Ck | X=x).</li>
              <li>将后验概率最大项作为样本所属类别.</li>
            </ol>
          </li>
        </ul>
      </li>
      <li><strong>注意事项</strong>:
        <ul>
          <li>朴素贝叶斯算法是监督学习的生成模型。</li>
          <li>对缺失数据不太敏感。</li>
          <li>能处理多分类任务。</li>
          <li>适合增量式训练（可以实时对新增样本进行训练）。</li>
          <li>在大量样本下会有较好的表现，对小规模数据仍然有效。</li>
          <li>由于可能出现某些类别在样本中没有出现导致条件概率为0的情况，会影响后验概率的估计，因此引入了<strong>拉普拉斯平滑</strong>来解决.</li>
        </ul>
      </li>
      <li><strong>条件概率的计算</strong>:
        <ul>
          <li>如果特征是离散值，可以假设其符合多项式分布，条件概率是在样本类别Ck中该特征出现的频率，可以通过拉普拉斯平滑解决0概率问题.</li>
          <li>如果特征是稀疏二项离散值，可以假设其符合伯努利分布.</li>
          <li>如果特征是连续值，通常取其先验概率为正态分布，条件概率通过正态分布公式计算.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 六、支持向量机（Support Vector Machine, SVM） -->
  <div class="card">
    <h2>六、支持向量机（Support Vector Machine, SVM）</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>是一种基于统计学习理论的监督学习方法.</li>
          <li>可用于多分类问题.</li>
          <li>优化目标是找到一个超平面，使得空间中距离超平面最近的点（支持向量）到超平面的集合间隔尽可能大.</li>
          <li>超平面的位置仅由支持向量决定，与其他样本点无关.</li>
          <li>通过核技巧（kernel trick）以及软间隔最大化解决非线性分类器的问题，支持非线性的核函数.</li>
          <li>在解决小样本、非线性及高维模式识别等问题中表现出许多特有优势.</li>
          <li><strong>解决不同类型问题</strong>:
            <ul>
              <li><strong>线性可分</strong>: 通过硬间隔最大化，学习一个线性可分支持向量机.</li>
              <li><strong>近似线性可分</strong>: 通过软间隔最大化，学习一个线性支持向量机.</li>
              <li><strong>线性不可分</strong>: 通过使用核技巧以及软间隔最大化，学习一个非线性支持向量机.</li>
            </ul>
          </li>
          <li><strong>常用核函数</strong>: 线性核函数、多项式核函数、径向基核函数（高斯核）、Sigmoid核函数.</li>
        </ul>
      </li>
    </ul>
  </div>

 <!-- 七、集成学习（Ensemble Learning） -->
  <div class="card">
    <h2>七、集成学习（Ensemble Learning）</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>不是一个单独的机器学习算法，而是将多个机器学习模型（弱学习器）组合起来解决问题，从而有效地提升分类效果，形成一个强学习器.</li>
          <li>基本思想是先通过一定规则生成个体学习器（基学习器），再采用某种集成策略将预测结果组合起来.</li>
          <li>基学习器可以是同质的（如CART决策树、神经网络），也可以是异质的.</li>
          <li><strong>主要类型</strong>:
            <ul>
              <li><strong>Bagging（Bootstrap aggregating）</strong>: 各基分类器之间不存在强依赖关系，可以并行生成基分类器. 通过有放回采样获取每个模型的样本集合，训练出的每个模型独立同分布. 主要目的是降低模型的方差（variance）. 随机森林是基于Bagging框架设计的.</li>
              <li><strong>Boosting</strong>: 个体学习器之间存在强依赖关系，串行生成个体学习器. 原理是利用基学习器之间的依赖关系，对之前训练中错误标记的样本赋以较高的权重值，以提高整体的预测效果. 主要目的是降低模型的偏差（bias）. AdaBoost和Gradient Boosting（如GBDT）是其代表算法. Boosting中的基分类器可以是不同类的分类器.</li>
              <li><strong>Stacking</strong>: 用不同的子模型对输入提取不同的特征，然后拼接成一个特征向量，在特征空间再训练一个学习器进行预测.</li>
            </ul>
          </li>
          <li><strong>组合策略</strong>:
            <ul>
              <li>分类任务中，通常采用投票法（绝对多数或相对多数）.</li>
              <li>回归任务中，通常通过简单平均或加权平均产生集成输出.</li>
            </ul>
          </li>
          <li>集成学习的效果依赖于基学习器之间的差异性（多样性）. 提升基学习器的多样性是集成学习的关键策略.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 八、主成分分析（Principal Component Analysis, PCA） -->
  <div class="card">
    <h2>八、主成分分析（Principal Component Analysis, PCA）</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>是一种经典的<strong>线性降维分析算法</strong>.</li>
          <li>目标是通过某种线性投影，将高维数据映射到低维空间中，并期望在所投影的维度上数据的方差最大.</li>
          <li>通过旋转坐标系将数据在新的坐标系下表示，并省略信息量太少（方差极小）的轴，从而达到降维目的.</li>
          <li>对于特征变量，使用样本集合中对应子变量上取值的方差来表示该特征的重要程度. <strong>方差越大，特征的重要程度越高；方差越小，特征的重要程度越低</strong>.</li>
          <li>降维的目的是降低数据的维度，从而方便后续对数据的存储、可视化、建模等操作.</li>
          <li>能减少冗余信息造成的误差，提高识别精度，并寻找数据内部的本质结构特征.</li>
          <li>各主成分之间正交，可消除原始数据成分间的相互影响.</li>
          <li>计算方法简单，易于实现.</li>
          <li><strong>局限性</strong>: 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强. 方差小的非主成分也可能含有对样本差异的重要信息，降维丢弃的数据可能对后续数据处理有影响.</li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 九、聚类 K-Means -->
  <div class="card">
    <h2>九、聚类 K-Means</h2>
    <ul>
      <li><strong>特点</strong>:
        <ul>
          <li>聚类是一种<strong>无监督学习</strong>算法.</li>
          <li>聚类的宗旨是<strong>类内距离最小化</strong>和<strong>类间距离最大化</strong>.</li>
          <li>K-Means算法是一种<strong>原型聚类</strong>算法，聚类结果能够通过一组原型（质心）刻画.</li>
          <li>K-Means算法在一定条件下可以收敛.</li>
          <li><strong>算法步骤</strong>: 随机选择K个对象作为初始聚类中心，对待分类的模式特征矢量逐个按最小距离原则分划给K类，重新计算每个聚类的均值，循环直到每个聚类不再发生变化.</li>
          <li><strong>优点</strong>:
            <ul>
              <li>算法简单，易于实现.</li>
              <li>当类密集，且类与类之间区别明显（比如球型聚集）时，聚类效果很好.</li>
              <li>对处理大数据集高效，复杂度是O(Nkt).</li>
            </ul>
          </li>
          <li><strong>缺点</strong>:
            <ul>
              <li>结果与初始质心有关（可能达到局部最小值，而不是全局最优解）. K-means++算法是为解决初始值敏感问题而提出.</li>
              <li>必须预先给出聚类的类别数k.</li>
              <li>对“噪声”和孤立点数据敏感，少量这些数据对平均值产生较大影响.</li>
              <li>不适合发现非凸面形状的聚类.</li>
              <li>在大数据集上收敛较慢.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </div>

  <!-- 十、神经网络与深度学习 -->
  <div class="card">
    <h2>十、神经网络与深度学习</h2>
    <ul>
      <li><strong>神经网络与深度学习</strong>:
        <ul>
          <li>神经网络的基本组成单位是神经元模型.</li>
          <li>深度学习从某种程度上模拟了人类逐层进行、逐步抽象的认知过程.</li>
          <li>深层神经网络在神经元数目一定的情况下，相比于传统浅层神经网络来说，具有更强大的学习能力，能够从原始输入中自动提取出具有高度抽象含义的特征.</li>
          <li>深度学习多层表示的好处是能够用较少的参数表征十分复杂的函数.</li>
          <li>卷积神经网络（CNN）和循环神经网络（RNN）是深度神经网络的两种重要类型.</li>
        </ul>
      </li>
      <li><strong>卷积（Convolution）操作</strong>:
        <ul>
          <li>卷积神经网络（CNN）大多数情况下用于处理计算机视觉相关的任务.</li>
          <li>利用卷积核（视为特征提取算子）在特征图上滑动，并计算卷积输出.</li>
          <li><strong>步长（Stride）</strong>: 卷积核每次移动的单位长度. 步长越大，特征矩阵越小.</li>
          <li><strong>填充（Padding）</strong>: 为原始数据填充一圈或几圈元素（常用补零），以保证相邻层的特征图具有相同长宽尺度，解决边界信息丢失问题.</li>
          <li><strong>特点</strong>: 局部连接和参数共享. 当前层每个神经元节点仅与上一层局部神经元节点连接，每个通道的所有神经元共享一个卷积核参数，大大降低模型复杂度，避免过拟合.</li>
        </ul>
      </li>
      <li><strong>池化（Pooling）操作</strong>:
        <ul>
          <li>目的在于<strong>降低当前特征图的维度</strong>.</li>
          <li>常见的池化方式有<strong>最大池化（Max Pooling）</strong>和<strong>均值池化（Average Pooling）</strong>. 还有最小池化.</li>
          <li>池化操作通过定义池化窗口的大小，从窗口内采样（取最大值、最小值或平均值）.</li>
        </ul>
      </li>
      <li><strong>激活函数（Activation Function）</strong>:
        <ul>
          <li>目的是进行<strong>非线性变换</strong>. 因为现实世界中的数据仅通过线性化建模往往不能够反映其规律.</li>
          <li><strong>常用激活函数</strong>: ReLU、Sigmoid、Tanh、Softmax.
            <ul>
              <li><strong>Sigmoid</strong>: 易于求导，输出区间固定（0到1），训练过程不易发散，可作为二分类概率输出函数.</li>
              <li><strong>ReLU</strong>: 广泛使用的一种激活函数.</li>
              <li><strong>Tanh</strong>: 使用Tanh的神经网络往往收敛更快.</li>
              <li><strong>Softmax</strong>: 常用于将函数输出转化为概率分布.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>典型神经网络的核心步骤（反向传播算法）</strong>:
        <ul>
          <li><strong>反向传播算法（Backpropagation, BP）</strong>是广泛使用的神经网络模型训练算法，又叫做梯度下降法.</li>
          <li><strong>工作过程</strong>: 包括<strong>信号的前向传播</strong>和<strong>误差的反向传播</strong>.
            <ul>
              <li><strong>前向传播</strong>: 输入信号通过隐含层作用于输出结点，产生输出信号.</li>
              <li><strong>反向传播</strong>: 若实际输出与期望输出不符，则将输出误差通过隐含层向输入层逐层反传，并将误差分摊给各层所有单元，以调整各单元权值和阈值.</li>
            </ul>
          </li>
          <li><strong>本质</strong>: 对各连接权值的动态调整.</li>
          <li><strong>优化过程</strong>: 反复迭代，通过损失函数和成本函数对前向传播结果进行判定，并通过后向传播过程对权重参数进行修正，直到满足终止条件.</li>
          <li>之所以称为反向传播，是由于在深层神经网络中，需要通过链式法则将梯度逐层传递到底层.</li>
        </ul>
      </li>
      <li><strong>神经网络优化过程中常出现的问题及相应的解决方案</strong>:
        <ul>
          <li><strong>常出现的问题</strong>:
            <ul>
              <li><strong>梯度消失（Vanishing Gradient）</strong>: 在BP算法中使用链式法则进行连乘时，靠近输入层的参数梯度几乎为0，导致参数更新几乎不发生变化，使得神经网络难以收敛. 常见原因如Sigmoid函数容易饱和.</li>
              <li><strong>梯度爆炸（Exploding Gradient）</strong>: 如果神经网络的参数初始化不合理，当连乘的梯度均大于1时，会导致底层参数的梯度过大，更新时参数无限增大，模型不稳定且不收敛.</li>
            </ul>
          </li>
          <li><strong>相应的解决方案</strong>:
            <ul>
              <li><strong>解决梯度消失</strong>: 更换激活函数（如选择ReLU），调整神经网络结构（减少层数）.</li>
              <li><strong>解决梯度爆炸</strong>: 模型参数初始化（如预训练），梯度裁剪（当梯度超过一定阈值时截断），参数正则化（对参数大小进行约束）.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </div>

</body>

</html>